# AI Collaboration Interview - Evaluation Guide

> **Note**: This document is for interviewer use only and should not be shared with candidates.

## Overview
This guide provides evaluation criteria for assessing a candidate's ability to effectively collaborate with AI coding assistants during the interview process. The evaluation focuses on four key areas: Prompt Engineering Skills, Critical Evaluation, Tool Utilization, and Technical Understanding.

## Scoring System
For each criterion, score candidates on a scale of 1-5:
1. Unsatisfactory - Significant improvement needed
2. Basic - Meets minimal expectations
3. Proficient - Meets expectations
4. Strong - Exceeds expectations
5. Exceptional - Far exceeds expectations

## Evaluation Categories

### 1. Prompt Engineering Skills

#### Specificity and clarity of prompts
- **1**: Vague, ambiguous prompts that frequently lead to irrelevant or incorrect responses
- **3**: Generally clear prompts that occasionally lack necessary specificity
- **5**: Consistently precise, well-structured prompts that clearly communicate intent and constraints

#### Context provided to AI assistant
- **1**: Minimal or irrelevant context provided, forcing AI to make assumptions
- **3**: Adequate context for basic tasks, but missing important details for complex problems
- **5**: Comprehensive context including requirements, constraints, and relevant code snippets

#### Use of iterative refinement
- **1**: Accepts initial AI responses without refinement
- **3**: Some refinement of AI responses, but may miss opportunities for improvement
- **5**: Systematic refinement through follow-up prompts, clarifications, and corrections

### 2. Critical Evaluation

#### Verification of AI-generated code
- **1**: Accepts and implements AI code without verification
- **3**: Basic verification of AI code, catches obvious errors
- **5**: Thorough verification including edge cases, performance considerations, and adherence to requirements

#### Adaptation of AI suggestions to specific needs
- **1**: Uses AI suggestions verbatim without adaptation
- **3**: Makes some adaptations to fit project requirements
- **5**: Skillfully adapts and integrates AI suggestions into the existing codebase with appropriate modifications

#### Recognition and correction of AI errors
- **1**: Fails to identify errors in AI-generated code
- **3**: Identifies obvious errors but may miss subtle issues
- **5**: Quickly identifies both obvious and subtle errors, effectively communicates issues to AI, and guides toward correct solutions

### 3. Tool Utilization

#### Effective use of AI-specific features
- **1**: Limited to basic text generation, unaware of AI capabilities
- **3**: Uses some advanced features but may not leverage full capabilities
- **5**: Expertly uses the full range of AI features including code explanation, refactoring, and debugging assistance

#### Balance between AI assistance and independent work
- **1**: Over-reliance on AI or reluctance to use AI at all
- **3**: Reasonable balance but may delegate inappropriate tasks or solve simple problems manually
- **5**: Optimal delegation of tasks based on AI strengths and limitations, focusing human effort where it adds most value

#### Time efficiency gains from AI collaboration
- **1**: AI collaboration slows down the development process
- **3**: Some efficiency gains in specific areas
- **5**: Significant overall productivity improvement through strategic AI collaboration

### 4. Technical Understanding

#### Ability to explain AI-generated code
- **1**: Cannot explain how AI-generated code works
- **3**: Basic understanding of AI-generated code functionality
- **5**: Comprehensive understanding with ability to explain implementation details and design decisions

#### Selection of appropriate problems for AI assistance
- **1**: Asks AI to solve problems ill-suited for AI capabilities
- **3**: Generally selects appropriate problems but may misjudge AI capabilities
- **5**: Consistently identifies optimal problems for AI assistance based on complexity, pattern-matching, and documentation needs

#### Integration of AI solutions with existing codebase
- **1**: Struggles to integrate AI solutions with existing code
- **3**: Successfully integrates AI solutions with some guidance
- **5**: Seamlessly integrates AI solutions while maintaining code consistency and project standards

## Challenge-Specific Evaluation Notes

### Bug Fixing Challenges
- Pay attention to how candidates describe bugs to the AI
- Note whether candidates verify fixes with tests
- Observe if candidates understand the root cause of bugs

### Feature Implementation Challenges
- Assess how candidates break down requirements for the AI
- Note whether candidates ensure new features integrate with existing architecture
- Observe if candidates implement proper error handling and validation

### Code Optimization Challenges
- Evaluate whether candidates can identify performance bottlenecks
- Note if candidates verify performance improvements with metrics
- Observe if candidates understand the tradeoffs in optimization decisions

### Documentation Gap Challenges
- Assess how candidates extract information from complex code
- Note whether candidates verify documentation accuracy
- Observe if candidates create documentation that balances completeness with clarity

## Overall Assessment

After evaluating all categories, provide an overall assessment:

- **Strong Hire**: Consistently scores 4-5 across most categories
- **Hire**: Scores 3-4 across most categories with no significant weaknesses
- **Borderline**: Mixed scores with some strengths and weaknesses
- **No Hire**: Consistently scores 1-2 across multiple categories

Include specific examples from the interview to support your assessment and highlight areas of strength and opportunities for improvement.
